{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMzJ62qjC/NYLJIP9ThOZ0T"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"XBkzFI3BlWwf"},"outputs":[],"source":["# src/softmax_regression.py\n","import numpy as np\n","from sklearn.metrics import f1_score\n","\n","class SoftmaxRegression:\n","    def __init__(self, input_dim, n_classes=3):\n","        limit = np.sqrt(6.0/(input_dim+n_classes))\n","        self.W = np.random.uniform(-limit, limit, (input_dim, n_classes)).astype(np.float32)\n","        self.b = np.zeros((1, n_classes), dtype=np.float32)\n","\n","    @staticmethod\n","    def softmax(z):\n","        z = z - z.max(axis=1, keepdims=True)\n","        e = np.exp(z, dtype=np.float32)\n","        return e/(e.sum(axis=1, keepdims=True)+1e-12)\n","\n","    def forward(self, X):\n","        return self.softmax(X @ self.W + self.b)\n","\n","    def predict(self, X):\n","        return self.forward(X).argmax(axis=1)\n","\n","    def fit(self, X, y, lr=0.2, epochs=8, batch_size=512,\n","            X_val=None, y_val=None, weight_decay=1e-4, patience=3, verbose=False):\n","        n, K = X.shape[0], self.b.shape[1]\n","        y_oh = np.eye(K, dtype=np.float32)[y]\n","        history = {\"loss\": [], \"val_loss\": [], \"val_acc\": [], \"val_f1\": []}\n","        best_val, wait, best_params = np.inf, 0, None\n","\n","        for ep in range(1, epochs+1):\n","            idx = np.random.permutation(n)\n","            Xb, yb = X[idx], y_oh[idx]\n","            for i in range(0, n, batch_size):\n","                xb = Xb[i:i+batch_size]\n","                yb_ = yb[i:i+batch_size]\n","                probs = self.forward(xb)\n","                grad_logits = (probs - yb_) / max(1, len(xb))\n","                dW = xb.T @ grad_logits\n","                db = grad_logits.sum(axis=0, keepdims=True)\n","                if weight_decay > 0:\n","                    dW += 2.0 * weight_decay * self.W\n","                self.W -= lr*dW\n","                self.b -= lr*db\n","\n","            # train loss\n","            p_tr = self.forward(X)\n","            ce = -np.mean(np.log(p_tr[np.arange(n), y]+1e-12))\n","            if weight_decay > 0:\n","                ce += weight_decay*(self.W**2).sum()/n\n","            history[\"loss\"].append(ce)\n","\n","            # val\n","            if X_val is not None and y_val is not None:\n","                pv = self.forward(X_val)\n","                ce_v = -np.mean(np.log(pv[np.arange(len(y_val)), y_val]+1e-12))\n","                if weight_decay > 0:\n","                    ce_v += weight_decay*(self.W**2).sum()/len(y_val)\n","                history[\"val_loss\"].append(ce_v)\n","                yvp = pv.argmax(axis=1)\n","                acc = (yvp == y_val).mean()\n","                f1  = f1_score(y_val, yvp, average=\"macro\")\n","                history[\"val_acc\"].append(acc); history[\"val_f1\"].append(f1)\n","\n","                if ce_v + 1e-6 < best_val:\n","                    best_val, wait = ce_v, 0\n","                    best_params = (self.W.copy(), self.b.copy())\n","                else:\n","                    wait += 1\n","                    if wait >= patience and best_params is not None:\n","                        self.W, self.b = best_params\n","                        if verbose:\n","                            print(f\"[Softmax][ES] ep {ep}/{epochs} val_loss={ce_v:.4f}\")\n","                        break\n","        return history\n"]}]}