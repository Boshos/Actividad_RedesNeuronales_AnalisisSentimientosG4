{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMsBN1eeML4qnhQ6ZTCSLDZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"WVxiIB7VlNlg"},"outputs":[],"source":["# src/neural_network.py\n","import numpy as np\n","\n","# Activaciones y derivadas\n","ACTS = {\n","    \"relu\": (\n","        lambda z: np.maximum(0, z).astype(np.float32),\n","        lambda z: (z > 0).astype(np.float32),\n","    ),\n","    \"tanh\": (\n","        lambda z: np.tanh(z).astype(np.float32),\n","        lambda z: (1 - np.tanh(z)**2).astype(np.float32),\n","    ),\n","    \"sigmoid\": (\n","        lambda z: (1/(1+np.exp(-z))).astype(np.float32),\n","        lambda z: (1/(1+np.exp(-z))) * (1 - (1/(1+np.exp(-z)))),\n","    )\n","}\n","\n","def one_hot(y, n_classes):\n","    oh = np.zeros((y.shape[0], n_classes), dtype=np.float32)\n","    oh[np.arange(y.shape[0]), y] = 1.0\n","    return oh\n","\n","class NeuralNetwork:\n","    \"\"\"\n","    MLP desde cero (NumPy).\n","    - Inicialización He para capas ReLU, Xavier para otras.\n","    - Softmax + CrossEntropy al final.\n","    - SGD mini-batch con weight decay (L2) y early stopping simple (val_acc).\n","    layers: [input_dim, h1, h2, ..., n_classes]\n","    activation: 'relu' | 'tanh' | 'sigmoid' (para ocultas)\n","    \"\"\"\n","    def __init__(self, layers, activation=\"relu\"):\n","        assert len(layers) >= 3, \"Mínimo: input, al menos 1 oculta, y salida\"\n","        self.layers   = layers\n","        self.act_name = activation\n","        self.params   = {}\n","        self._init_params()\n","\n","    def _init_params(self):\n","        # He para ReLU; Xavier para otras\n","        for l in range(1, len(self.layers)):\n","            fan_in, fan_out = self.layers[l-1], self.layers[l]\n","            if l < len(self.layers)-1 and self.act_name == \"relu\":\n","                W = np.random.randn(fan_in, fan_out).astype(np.float32) * np.sqrt(2.0/fan_in)\n","            else:\n","                limit = np.sqrt(6.0/(fan_in+fan_out))\n","                W = np.random.uniform(-limit, limit, (fan_in, fan_out)).astype(np.float32)\n","            b = np.zeros((1, fan_out), dtype=np.float32)\n","            self.params[f\"W{l}\"] = W\n","            self.params[f\"b{l}\"] = b\n","\n","    @staticmethod\n","    def softmax(z):\n","        z = z - z.max(axis=1, keepdims=True)\n","        e = np.exp(z, dtype=np.float32)\n","        return e/(e.sum(axis=1, keepdims=True)+1e-12)\n","\n","    def forward(self, X):\n","        A = X\n","        caches = []\n","        act, _ = ACTS[self.act_name]\n","        # ocultas\n","        for l in range(1, len(self.layers)-1):\n","            W, b = self.params[f\"W{l}\"], self.params[f\"b{l}\"]\n","            Z = A @ W + b\n","            A = act(Z)\n","            caches.append((A, Z, W, b))\n","        # salida\n","        L = len(self.layers)-1\n","        W, b = self.params[f\"W{L}\"], self.params[f\"b{L}\"]\n","        logits = A @ W + b\n","        probs = self.softmax(logits)\n","        return probs, caches, (A, W, b)\n","\n","    def fit(self, X, y, lr=0.03, epochs=20, batch_size=256,\n","            X_val=None, y_val=None, weight_decay=1e-4, patience=3, verbose=False):\n","        n, K = X.shape[0], self.layers[-1]\n","        y_oh = one_hot(y, K)\n","        hist = {\"loss\": [], \"val_acc\": []}\n","        act, dact = ACTS[self.act_name]\n","        best_val, best_params, wait = -1.0, None, 0\n","\n","        for ep in range(1, epochs+1):\n","            idx = np.random.permutation(n)\n","            Xb, yb, y_ohb = X[idx], y[idx], y_oh[idx]\n","\n","            for i in range(0, n, batch_size):\n","                X_batch = Xb[i:i+batch_size]\n","                y_batch = y_ohb[i:i+batch_size]\n","\n","                probs, caches, last = self.forward(X_batch)\n","                # grad salida (CE + softmax)\n","                grad = (probs - y_batch)/max(1, len(X_batch))\n","\n","                # backprop capa salida\n","                A_prev, W_L, b_L = last\n","                dW = A_prev.T @ grad + weight_decay * W_L\n","                db = grad.sum(axis=0, keepdims=True)\n","                self.params[f\"W{len(self.layers)-1}\"] -= lr*dW\n","                self.params[f\"b{len(self.layers)-1}\"] -= lr*db\n","\n","                # backprop ocultas\n","                dA = grad @ W_L.T\n","                for l in range(len(self.layers)-2, 0, -1):\n","                    A_l, Z_l, W_l, b_l = caches[l-1]\n","                    A_prev = X_batch if l == 1 else caches[l-2][0]\n","                    dZ = dA * dact(Z_l)\n","                    dW = A_prev.T @ dZ + weight_decay * W_l\n","                    db = dZ.sum(axis=0, keepdims=True)\n","                    self.params[f\"W{l}\"] -= lr*dW\n","                    self.params[f\"b{l}\"] -= lr*db\n","                    if l > 1:\n","                        dA = dZ @ W_l.T\n","\n","            # métrica simple por época\n","            probs_train, _, _ = self.forward(X)\n","            loss = -np.mean(np.log(probs_train[np.arange(n), y]+1e-12))\n","            hist[\"loss\"].append(loss)\n","\n","            if X_val is not None:\n","                y_pred = self.predict(X_val)\n","                val_acc = (y_pred == y_val).mean()\n","                hist[\"val_acc\"].append(val_acc)\n","                if val_acc > best_val:\n","                    best_val, wait = val_acc, 0\n","                    best_params = {k:v.copy() for k,v in self.params.items()}\n","                else:\n","                    wait += 1\n","                    if wait >= patience and best_params is not None:\n","                        self.params = best_params\n","                        if verbose:\n","                            print(f\"[NN] EarlyStopping @ epoch {ep} | best_val_acc={best_val:.4f}\")\n","                        break\n","\n","            if verbose and hist[\"val_acc\"]:\n","                print(f\"[NN {self.act_name}] ep {ep}/{epochs} | val_acc={hist['val_acc'][-1]:.4f}\")\n","\n","        return hist\n","\n","    def predict(self, X):\n","        probs, _, _ = self.forward(X)\n","        return probs.argmax(axis=1)\n"]}]}